{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob \n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "def match_names(wrong_names,popped_names,list_name,file_name):\n",
    "    matches = []\n",
    "    for w in wrong_names:\n",
    "        #print(str(w[1]))\n",
    "        x = process.extractOne(w[0],popped_names)\n",
    "        balance= w[1]\n",
    "        currency = w[2]\n",
    "        if x[1] > 88:\n",
    "            matches.append(tuple([w[0],x[0],balance, currency,  list_name,file_name]))\n",
    "    #print(matches)    \n",
    "    return matches\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def search_df(data,df1_active):\n",
    "  \n",
    "    for i in range(0,len(data)):\n",
    "        found = df1_active[df1_active['organizationName'].astype(str).str.contains(data[i][0])].values\n",
    "        print(found)\n",
    "        if found.size > 0 :\n",
    "            data[i].append(\"Active\")\n",
    "        else:\n",
    "            data[i].append('inactive')\n",
    "    pprint(data)\n",
    "        \n",
    "\n",
    "    \n",
    "def junk():\n",
    "    pass\n",
    "    '''\n",
    "    df1 = df1['organizationName'].str.strip()\n",
    "    print(df1.loc[78])\n",
    "    df2 = df2['organizationName'].str.strip()\n",
    "    df3 = df3['organizationName'].str.strip()\n",
    "    print(df3.loc[173])\n",
    "    merge_df_1 = pd.merge(df1,df2)\n",
    "    merge_df_2 = pd.merge(df1,df3)\n",
    "    merge_df_1['Actor']= 'AccountParadise'\n",
    "    merge_df_1['data_file']= 'icare_customers.csv'\n",
    "    #if merge_df_1.empty == False:\n",
    "\n",
    "        #print(merge_df_1)\n",
    "                #print('{}->{}->{}'.format(pd.merge(df1,df2),'AccountParadise',sheet_name))\n",
    "    #elif  merge_df_2.empty == False:\n",
    "    merge_df_2['Actor']= 'Luxury187'\n",
    "    merge_df_2['data_file']= 'icare_customers.csv'\n",
    "    #print(merge_df_2)\n",
    "                #print('{}->{}->{}'.format(pd.merge(df1,df3),'Luxury187',sheet_name))\n",
    "    #print(merge_df_1)\n",
    "    #print(merge_df_2)\n",
    "    #print('{}->{}'.format(pd.merge(df1,df2),'AccountParadise'))\n",
    "    #print('{}->{}'.format(pd.merge(df1,df3),'Luxury187'))\n",
    "    '''\n",
    "    '''\n",
    "choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"]\n",
    "print(process.extract(\"new york jets\", choices))\n",
    "df2['key']=df2.organizationName.apply(lambda x : [process.extract(x, df1.organizationName, limit=1)][0][0][0])\n",
    "#print(pd.merge(df1,df2,left_on='key', right_on='organizationName'))\n",
    "'''\n",
    "##PROD2_GL_liabilities_Qry4.csv\n",
    "df1_liab = pd.read_csv(\"PROD1_GL_liabilities_Qry4.csv\", usecols=[0,1,2])\n",
    "df1_active = pd.read_csv(\"PROD1_GL_orgs_lastBusDate_activeCust.csv\", usecols=[0])\n",
    "print(df1_active.head())\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob \n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "def match_names(wrong_names,popped_names,list_name,file_name):\n",
    "    matches = []\n",
    "    for w in wrong_names:\n",
    "        #print(str(w[1]))\n",
    "        x = process.extractOne(w[0],popped_names)\n",
    "        balance= w[1]\n",
    "        currency = w[2]\n",
    "        if x[1] > 88:\n",
    "            matches.append(tuple([w[0],x[0],balance, currency,  list_name,file_name]))\n",
    "    #print(matches)    \n",
    "    return matches\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def search_df(data,df1_active):\n",
    "  \n",
    "    for i in range(0,len(data)):\n",
    "        found = df1_active[df1_active['organizationName'].astype(str).str.contains(data[i][0])].values\n",
    "        print(found)\n",
    "        if found.size > 0 :\n",
    "            data[i].append(\"Active\")\n",
    "        else:\n",
    "            data[i].append('inactive')\n",
    "    pprint(data)\n",
    "        \n",
    "\n",
    "    \n",
    "def junk():\n",
    "    pass\n",
    "    '''\n",
    "    df1 = df1['organizationName'].str.strip()\n",
    "    print(df1.loc[78])\n",
    "    df2 = df2['organizationName'].str.strip()\n",
    "    df3 = df3['organizationName'].str.strip()\n",
    "    print(df3.loc[173])\n",
    "    merge_df_1 = pd.merge(df1,df2)\n",
    "    merge_df_2 = pd.merge(df1,df3)\n",
    "    merge_df_1['Actor']= 'AccountParadise'\n",
    "    merge_df_1['data_file']= 'icare_customers.csv'\n",
    "    #if merge_df_1.empty == False:\n",
    "\n",
    "        #print(merge_df_1)\n",
    "                #print('{}->{}->{}'.format(pd.merge(df1,df2),'AccountParadise',sheet_name))\n",
    "    #elif  merge_df_2.empty == False:\n",
    "    merge_df_2['Actor']= 'Luxury187'\n",
    "    merge_df_2['data_file']= 'icare_customers.csv'\n",
    "    #print(merge_df_2)\n",
    "                #print('{}->{}->{}'.format(pd.merge(df1,df3),'Luxury187',sheet_name))\n",
    "    #print(merge_df_1)\n",
    "    #print(merge_df_2)\n",
    "    #print('{}->{}'.format(pd.merge(df1,df2),'AccountParadise'))\n",
    "    #print('{}->{}'.format(pd.merge(df1,df3),'Luxury187'))\n",
    "    '''\n",
    "    '''\n",
    "choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"]\n",
    "print(process.extract(\"new york jets\", choices))\n",
    "df2['key']=df2.organizationName.apply(lambda x : [process.extract(x, df1.organizationName, limit=1)][0][0][0])\n",
    "#print(pd.merge(df1,df2,left_on='key', right_on='organizationName'))\n",
    "'''\n",
    "##PROD2_GL_liabilities_Qry4.csv\n",
    "df1_liab = pd.read_csv(\"PROD1_GL_liabilities_Qry4.csv\", usecols=[0,1,2])\n",
    "df1_active = pd.read_csv(\"PROD1_GL_orgs_lastBusDate_activeCust.csv\", usecols=[0])\n",
    "#print(df1_liab.head())\n",
    "df2_1 = pd.read_csv(\"test.csv\",)\n",
    "#print(list(df2_1.columns))\n",
    "df2 = pd.read_csv(\"test.csv\", usecols=[0])\n",
    "df3 = pd.read_csv(\"Luxury187_Restaurants.csv\", usecols=[0])\n",
    "#B.columns = B.columns.str.replace(' ','')\n",
    "#print(A)\n",
    "liab_names = df1_liab.dropna().values\n",
    "liab_names = liab_names.tolist()\n",
    "#liab_names[0].append(\"test\")\n",
    "print(liab_names[0])\n",
    "#search_df(liab_names,df1_active)\n",
    "found = df1_active[df1_active['organizationName'].astype(str).str.contains('Epoch Restaurant Group')].values\n",
    "\n",
    "\n",
    "luxury_names = df3['organizationName'].values\n",
    "#luxury_matches = match_names(wrong_names,luxury_names,'luxury187','test_file.csv')\n",
    "acct_paradise_names = df2['organizationName'].values\n",
    "#acct_paradise_matches = match_names(wrong_names,acct_paradise_names,'acctParadise','test_file.csv')\n",
    "\n",
    "#df1['luxury187']= pd.Series(luxury_matches)\n",
    "#df1['AcctPara'] = pd.Series(acct_paradise_matches)\n",
    "#df1['source_file'] = \"test_dataset.csv\"\n",
    "labels = ['customer_name','Matched_item','liability_amt','currency', 'actor_list', 'source_file']\n",
    "\n",
    "#df4 = pd.DataFrame.from_records(luxury_matches,columns=labels)\n",
    "#df5 = pd.DataFrame.from_records(acct_paradise_matches,columns=labels)\n",
    "#df4 = df4.append(df5, ignore_index = True)\n",
    "#df4.loc[len(df4)] = pd.Series(acct_paradise_matches)\n",
    "#print(df1)\n",
    "#print(df5)\n",
    "\n",
    "#df4.to_csv('PROD1_GL_liabilities_final_report.csv')\n",
    "\n",
    "#df2 = pd.DataFrame(B)\n",
    "#df3 = pd.DataFrame(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import csv \n",
    "from collections import defaultdict \n",
    "\n",
    "\n",
    "data_dict = defaultdict(dict)\n",
    "\n",
    "folio_df = pd.read_csv('folio_data.csv',usecols=['Folio_number','Name','Email'])\n",
    "\n",
    "folio_df = folio_df.replace(to_replace='(b\\'|\\')', value='',regex=True)\n",
    "folio_df = folio_df.replace(to_replace='(.*<|>|\\\")', value='',regex=True)\n",
    "[data_dict[email].update({'Email': email, 'Folio': set() })  for email in folio_df['Email']]\n",
    "for index,row in folio_df.iterrows():\n",
    "    data_dict[row['Email']]['Folio'].add(row['Folio_number']) \n",
    "    #data_dict[row['Name']]['Email'].add(row['Email']) \n",
    "\n",
    "final_df = pd.DataFrame(data_dict)\n",
    "final_df = final_df.transpose()\n",
    "final_df = final_df[['Email', 'Folio']]\n",
    "final_df.to_csv('f_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlrd import open_workbook\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process \n",
    "import csv\n",
    "import glob\n",
    "from pprint import pprint\n",
    "import jellyfish\n",
    "liab_files = glob.glob(\"*liabilities*.csv\")\n",
    "active_files = glob.glob(\"*activeCust*.csv\")\n",
    "xls_files= glob.glob(\"*.xlsx\")\n",
    "acctparadise =  pd.read_csv(\"test.csv\", usecols=[0])\n",
    "luxury187 = pd.read_csv(\"Luxury187_Restaurants.csv\", usecols=[0])\n",
    "\n",
    "def match_names(wrong_names,popped_names,list_name,file_name):\n",
    "    matches = []\n",
    "    for w in wrong_names:\n",
    "        #print(str(w[1]))\n",
    "        x = process.extractOne(w[0],popped_names)\n",
    "        balance= w[1]\n",
    "        currency = w[2]\n",
    "        active = w[3]\n",
    "        if x[1] > 88:\n",
    "            matches.append(tuple([w[0],x[0],balance, currency,active,list_name,file_name]))\n",
    "    #print(matches)    \n",
    "    return matches\n",
    "\n",
    "def search_df(data,df1_active):\n",
    "  \n",
    "    for i in range(0,len(data)):\n",
    "        found = df1_active[df1_active['organizationName'].astype(str).str.contains(data[i][0])].values\n",
    "        if found.size > 0 :\n",
    "            data[i].append(\"Active\")\n",
    "        else:\n",
    "            data[i].append('inactive')\n",
    "    return data\n",
    "\n",
    "\n",
    "def xls_csv():\n",
    "    for file in xls_files:\n",
    "\n",
    "        sheet_name = file.split('/')[7].replace('.xlsx','')\n",
    "\n",
    "        wb = open_workbook(file)\n",
    "\n",
    "        sheet = wb.sheet_by_index(0)\n",
    "        with open(\"%s.csv\" %(sheet_name), \"w\") as file:\n",
    "                writer = csv.writer(file, delimiter = \",\")\n",
    "                print(sheet, sheet.name, sheet.ncols, sheet.nrows)\n",
    "                header = [cell.value for cell in sheet.row(0)]\n",
    "                writer.writerow(header)\n",
    "                for row_idx in range(1, sheet.nrows):\n",
    "                    row = [int(cell.value) if isinstance(cell.value, float) else cell.value\n",
    "                           for cell in sheet.row(row_idx)]\n",
    "                    writer.writerow(row)\n",
    "\n",
    "                    \n",
    "\n",
    "        \n",
    "        \n",
    "def process_dframes():\n",
    "    labels = ['customer_name','Matched_item','liability_amt','currency','is_active','actor_list', 'source_file']\n",
    "    final_df = pd.DataFrame(columns=labels)\n",
    " \n",
    "    #print(indices)\n",
    "    for i  in range(0,len(liab_files)):\n",
    "        \n",
    "        #print('procession file: {} -> {}'.format(liab_files[i],active_files[i]))\n",
    "        \n",
    "        try:\n",
    "            sheet_name = liab_files[i].split('/')[7]\n",
    "            #print(sheet_name)\n",
    "            prefix = sheet_name.split('_')[0]\n",
    "            index = [i for i, s in enumerate(active_files) if prefix in s]\n",
    "            #print(type(indices[0]))\n",
    "            #print('{}-> {}'.format(liab_files[i],active_files[index[0]]))\n",
    "            \n",
    "            cust_df = pd.read_csv(liab_files[0], usecols=[0,1,2] ,encoding= \"ISO-8859-1\")\n",
    "            #index = active_files.index(liab_files[0])\n",
    "            #print(index)\n",
    "            #active \n",
    "            active_df = pd.read_csv(active_files[index[0]], usecols=[0] ,encoding= \"ISO-8859-1\")\n",
    "            #cust_df = pd.read_csv(file, usecols=[0] ,encoding= \"ISO-8859-1\")\n",
    "            df1 = pd.DataFrame(cust_df)\n",
    "            df2 = pd.DataFrame(acctparadise)\n",
    "            df3 = pd.DataFrame(luxury187)\n",
    "            df1_names = df1.dropna().values\n",
    "            df1_names = df1_names.tolist()\n",
    "            df1_names = search_df(df1_names,active_df)\n",
    "            df2_names = df2['organizationName'].values\n",
    "            #print(df1_names)\n",
    "            \n",
    "            df2_matches = match_names(df1_names,df2_names,'acctParadise',sheet_name)\n",
    "            df3_names = df3['organizationName'].values\n",
    "            df3_matches = match_names(df1_names,df3_names,'luxury187',sheet_name)\n",
    "            print(df3_matches)        \n",
    "            #df1 = df1['organizationName'].str.strip()\n",
    "            #df2 = df2['organizationName'].str.strip()\n",
    "            #df3 = df3['organizationName'].str.strip()\n",
    "\n",
    "            #df1['luxury187']= pd.Series(luxury_matches)\n",
    "            #df1['AcctPara'] = pd.Series(acct_paradise_matches)\n",
    "            #df1['source_file'] = \"test_dataset.csv\"\n",
    "\n",
    "\n",
    "            df4 = pd.DataFrame.from_records(df3_matches)\n",
    "            df5 = pd.DataFrame.from_records(df2_matches)\n",
    "            df4 = df4.append(df5, ignore_index = True)\n",
    "            #df4.loc[len(df4)] = pd.Series(acct_paradise_matches)\n",
    "            if df4.empty == False:\n",
    "                print(df4)\n",
    "                final_df = final_df.append(df4)\n",
    "            \n",
    "        except Exception as ex :\n",
    "            print(ex)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    final_df.to_csv('icare_final_report.csv')\n",
    "    '''\n",
    "        merge_df_1 = pd.merge(df1,df2)\n",
    "        merge_df_2 = pd.merge(df1,df3)\n",
    "        \n",
    "        #df2.index = df2.index.map(lambda x: get_closest_match(x, df1.index.value))\n",
    "        \n",
    "        #df1.join(df2)\n",
    "        print(df1.index.values[0])\n",
    "        \n",
    "        \n",
    "        print(merge_df_1)\n",
    "        print(merge_df_2)\n",
    "        if merge_df_1.empty == False:\n",
    "            merge_df_1['Actor']= 'AccountParadise'\n",
    "            merge_df_1['data_file']= sheet_name\n",
    "            print(merge_df_1)\n",
    "            #print('{}->{}->{}'.format(pd.merge(df1,df2),'AccountParadise',sheet_name))\n",
    "        elif  merge_df_2.empty == False:\n",
    "            merge_df_2['Actor']= 'Luxury187'\n",
    "            merge_df_2['data_file']= sheet_name\n",
    "            print(merge_df_2)\n",
    "            #print('{}->{}->{}'.format(pd.merge(df1,df3),'Luxury187',sheet_name))\n",
    "        '''\n",
    "   \n",
    "#xls_csv()        \n",
    "process_dframes()                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ipinfo \n",
    "def get_loc(ip):\n",
    "    access_token = '84c86bdd9f8f9c'\n",
    "    ip_handler = ipinfo.getHandler(access_token)\n",
    "    try:\n",
    "        loc = ip_handler.getDetails(ip)\n",
    "    except:\n",
    "        return pd.np.nan\n",
    "    try:\n",
    "        return loc.loc if loc else pd.np.nan\n",
    "    except:\n",
    "        return pd.np.nan\n",
    "    \n",
    "\n",
    "                \n",
    "i = pd.read_csv(r'arcane_victim_ips.txt',usecols=[0],header=None)\n",
    "uniq_ips = i[0].unique()\n",
    "uniq_ips = pd.Series(uniq_ips, index= uniq_ips)\n",
    "i['coordinates'] = i[0].map(uniq_ips.apply(get_loc))\n",
    "\n",
    "i.to_csv('arcane_victim_ips_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import io\n",
    "import csv \n",
    "import socket\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "def get_hostname(ip):\n",
    "    try:\n",
    "        hostname = socket.gethostbyaddr(ip)\n",
    "        return hostname[0]\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "    \n",
    "def process_line(reader,all):\n",
    "    for row in reader:\n",
    "        ip = row[7]\n",
    "        hostname = get_hostname(ip)\n",
    "        row.append(hostname)\n",
    "        all.append(row)\n",
    "    return \"completed\"\n",
    "    \n",
    "    \n",
    "    \n",
    "def multiprocess_results(threads,reader,all):\n",
    "    pool =  ThreadPool(threads)\n",
    "    results = pool.map(process_line, reader,all)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "        \n",
    "with open('file.csv','r') as fh_reader: \n",
    "    \n",
    "    with open('TIR003-fixed.csv','w') as fh_writer: \n",
    "        reader = csv.reader(fh_reader)\n",
    "        writer = csv.writer(fh_writer,lineterminator='\\n',delimiter=',')\n",
    "        all = []\n",
    "        row = next(reader)\n",
    "        row.append('reverse_dns')\n",
    "        all.append(row)\n",
    "\n",
    "        result = multiprocess_results(10,reader,all)\n",
    "        for i in result:\n",
    "            print(r)\n",
    "        \n",
    "        writer.writerows(all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "regr = LinearRegression()\n",
    "df = pd.read_csv(\"https://s3.amazonaws.com/codecademy-content/programs/data-science-path/linear_regression/honeyproduction.csv\")\n",
    "\n",
    "prod_per_year = df.groupby('year').totalprod.mean().reset_index()\n",
    "\n",
    "X = prod_per_year['year']\n",
    "X = X.values.reshape(-1,1)\n",
    "y = prod_per_year['totalprod']\n",
    "X_future = np.array(range(2013,2050))\n",
    "X_future = X_future.reshape(-1,1)\n",
    "\n",
    "regr.fit(X,y)\n",
    "y_predict = regr.predict(X)\n",
    "future_predict = regr.predict(X_future)\n",
    "plt.scatter(X,y_predict)\n",
    "plt.scatter(X_future,future_predict)\n",
    "plt.show()\n",
    "\n",
    "print(regr.coef_)\n",
    "print(regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# load and investigate the data here:\n",
    "\n",
    "dataset = pd.read_csv('tennis_stats.csv')\n",
    "\n",
    "#print(dataset.head())\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# perform exploratory analysis here:\n",
    "\n",
    "x = df[['BreakPointsOpportunities']]\n",
    "y = df[['Winnings']]\n",
    "\n",
    "plt.scatter(x,y,alpha=0.4)\n",
    "plt.xlabel(\"break points opportunities\")\n",
    "plt.ylabel(\"winnings\")\n",
    "plt.title(\"relationships b/w breakpoints and winning\")\n",
    "plt.show()\n",
    "plt.clf\n",
    "## perform single feature linear regressions here:\n",
    "\n",
    "features_train,features_test,outcome_train, outcome_test = train_test_split(x,y,train_size=0.8,test_size=0.2)\n",
    "model = LinearRegression()\n",
    "model.fit(features_train,outcome_train)\n",
    "model.score(features_test,outcome_test)\n",
    "\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "plt.scatter(outcome_test,prediction, alpha=0.4)\n",
    "plt.title('Predicted Winnings vs. Actual Winnings - 1 Feature')\n",
    "plt.xlabel('Actual Winnings')\n",
    "plt.ylabel('Predicted Winnings')\n",
    "plt.show()\n",
    "plt.clf\n",
    "\n",
    "\n",
    "## two feature linear regression. \n",
    "features = df[['BreakPointsOpportunities',\n",
    "'FirstServeReturnPointsWon']]\n",
    "\n",
    "outcome = df[['Winnings']] \n",
    "\n",
    "features_train1,features_test1,outcome_train1, outcome_test1 = train_test_split(features,outcome,train_size=0.8,test_size=0.2)\n",
    "model = LinearRegression()\n",
    "model.fit(features_train1,outcome_train1)\n",
    "model.score(features_test1,outcome_test1)\n",
    "\n",
    "prediction1 = model.predict(features_test1)\n",
    "\n",
    "plt.title('Predicted Winnings vs. Actual Winnings - 2 Features')\n",
    "plt.xlabel('Actual Winnings')\n",
    "plt.ylabel('Predicted Winnings')\n",
    "plt.scatter(outcome_test1,prediction1, alpha=0.4)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "features = df[['FirstServe','FirstServePointsWon','FirstServeReturnPointsWon',\n",
    "'SecondServePointsWon','SecondServeReturnPointsWon','Aces',\n",
    "'BreakPointsConverted','BreakPointsFaced','BreakPointsOpportunities',\n",
    "'BreakPointsSaved','DoubleFaults','ReturnGamesPlayed','ReturnGamesWon',\n",
    "'ReturnPointsWon','ServiceGamesPlayed','ServiceGamesWon','TotalPointsWon',\n",
    "'TotalServicePointsWon']]\n",
    "outcome = y\n",
    "\n",
    "\n",
    "features_train1,features_test1,outcome_train1, outcome_test1 = train_test_split(features,outcome,train_size=0.8,test_size=0.2)\n",
    "model = LinearRegression()\n",
    "model.fit(features_train1,outcome_train1)\n",
    "model.score(features_test1,outcome_test1)\n",
    "\n",
    "prediction1 = model.predict(features_test1)\n",
    "plt.title('Predicted Winnings vs. Actual Winnings - Multiple Features')\n",
    "plt.xlabel('Actual Winnings')\n",
    "plt.ylabel('Predicted Winnings')\n",
    "plt.scatter(outcome_test1,prediction1, alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandasql as ps\n",
    "\n",
    "data = pd.read_csv('Alerts_Endpoint_With_Emotet.csv')\n",
    "\n",
    "df = data.pivot_table(index=['Endpoint'], aggfunc='size')\n",
    "df.reset_index()\n",
    "query = \"select Endpoint, count(Endpoint) from data group by Endpoint;\"\n",
    "df2 = ps.sqldf(query, locals())\n",
    "df2.delete_index()\n",
    "df2 = df2.sort_values(df2.columns[1])\n",
    "print(df2.tail())\n",
    "df2.to_csv('out.csv',index=False) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandasql as pq \n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('ja3_fp.csv')\n",
    "#print(data.dtypes)\n",
    "#print(data['MatchOn'].iloc[0])\n",
    "print(data.columns)\n",
    "#print(data['MatchOn'].iloc[0])\n",
    "print(data['count_test'].head() )\n",
    "\n",
    "data['ja3_hash'] = data['MatchOn'].str.extract(r'(?=(\\b[A-Fa-f0-9]{32}\\b))')\n",
    "#data['malware_name'] = data['MatchOn'].str.extract(r'malware_name.*\\}')\n",
    "data['malware_family'] = data['MatchOn'].str.extract(r'malware_name\",\"Value\":\"(\\w+)\"')\n",
    "#print(data.head())\n",
    "group = data.groupby('ja3_hash')\n",
    "'''\n",
    "agg = group.count_test.sum().reset_index()\n",
    "agg = agg.sort_values(by=['count_test'], ascending=False)\n",
    "\n",
    "'''\n",
    "agg = group.aggregate({'count_test': np.sum , 'malware_family' : 'unique'})\n",
    "agg = agg.sort_values(by=['count_test'], ascending=False)\n",
    "\n",
    "#print(agg.iloc[1])\n",
    "agg.to_csv('ja3_fp_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandasql as pq \n",
    "\n",
    "\n",
    "df = pd.read_csv('file.csv')\n",
    "\n",
    "df_trans = df[(df['Transaction Type'] == 'RECORDKEEPING FEE')]\n",
    "df_trans['Amount'] = df_trans['Amount'].abs()\n",
    "df_trans.dtypes\n",
    "#print(df_trans.head())\n",
    "print(df_trans[df_trans['Amount'] <= 0.06])\n",
    "#print(df_trans.dtypes)\n",
    "df3 = df_trans[ df_trans['Amount'] >= 1.0]\n",
    "#print('\\n'.join(df3['Investment'].unique()))\n",
    "df3.to_csv('high_fees_offenders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load rankings data here:\n",
    "\n",
    "wood_df = pd.read_csv('Golden_Ticket_Award_Winners_Wood.csv')\n",
    "steel_df = pd.read_csv('Golden_Ticket_Award_Winners_Steel.csv')\n",
    "\n",
    "\n",
    "def rank_plot(coaster_name, df,park_name ):\n",
    "    coaster_rankings = df[(df['Name'] == coaster_name) & (df['Park'] == park_name)]\n",
    "    print(coaster_rankings.head())\n",
    "    rank = coaster_rankings['Rank']\n",
    "    year = coaster_rankings['Year of Rank']\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(\"Rank\")\n",
    "    plt.plot(year,rank)\n",
    "  \n",
    "    ax= plt.subplot()\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "def two_rank_plot(name,name2,df,pname,pname2):\n",
    "    coaster_rankings1 = df[(df['Name'] == name ) & (df['Park'] == pname)]\n",
    "    coaster_rankings2 = df[(df['Name'] == name2 ) & (df['Park'] == pname2)]\n",
    "    #print(coaster_rankings.head())\n",
    "    rank1 = coaster_rankings1['Rank']\n",
    "    rank2 = coaster_rankings2['Rank']\n",
    "    year = coaster_rankings1['Year of Rank']\n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(\"Rank\")\n",
    "    plt.plot(year,rank1)\n",
    "    plt.plot(year,rank2)\n",
    "    plt.legend([name,name2])\n",
    "    ax= plt.subplot()\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "def top_rank_plot(n,df):\n",
    "    top_ranks = df[df['Rank'] <= n]\n",
    "    rank = top_ranks['Rank']\n",
    "    year = top_ranks['Year of Rank']\n",
    "    ax= plt.subplot()\n",
    "    #plt.xlabel('Year')\n",
    "    #plt.ylabel(\"Rank\")\n",
    "    #plt.plot(year,rank)\n",
    "    for coaster in set(top_ranks['Name']):\n",
    "        coaster_ranking = top_ranks[top_ranks['Name'] == coaster ]\n",
    "        ax.plot(coaster_ranking['Year of Rank'],coaster_ranking['Rank'],label=coaster)\n",
    "    ax.invert_yaxis()\n",
    "    plt.legend(set(top_ranks['Name']))\n",
    "    \n",
    "#print(wood_df.head())   \n",
    "#print(wood_df[wood_df['Name'] == 'El Toro' ])\n",
    "\n",
    "#two_rank_plot(\"El Toro\",\"Boulder Dash\",wood_df, \"Six Flags Great Adventure\",\"Lake Compounce\")\n",
    "#top_rank_plot(3,wood_df)\n",
    "\n",
    "\n",
    "\n",
    "def plot_hist(df,column):\n",
    "    \n",
    "    data = df[df[column] >= 100 ].dropna()\n",
    "    plt.hist(data['speed'],histtype='step')\n",
    "    plt.show()\n",
    "    #print(data.head())\n",
    "\n",
    "\n",
    "def plot_bar(df,cname):\n",
    "    data = df[df['park'] == cname ]\n",
    "    print(data.head())\n",
    "    plt.barh(data['name'],data['num_inversions'])\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pie(df):\n",
    "    \n",
    "    open_df = df[df['status'] == 'status.operating']\n",
    "    closed_df = df[df['status'] == 'status.closed.definitely']\n",
    "    status_counts = [len(open_df), len(closed_df)]\n",
    "    labels = ['operating','closed']\n",
    "    plt.pie(status_counts, autopct='%0.1f%%', labels=labels )\n",
    "    plt.show()\n",
    "\n",
    "def plot_scatter(df,col1,col2):\n",
    "    df2 = df[df[col2] < 140 ]\n",
    "    height = df2[col2]\n",
    "    speed = df2[col1]\n",
    "    plt.scatter(speed.abs(),height.abs(),alpha=0.2)\n",
    "    #print(speed,height)\n",
    "       \n",
    "\n",
    "\n",
    "df2 = pd.read_csv('roller_coasters.csv')\n",
    "#print(df2.head())\n",
    "\n",
    "'''\n",
    "What roller coaster seating type is most popular? \n",
    "- sit down is the most popular \n",
    "And do different seating types result in higher/faster/longer roller coasters?\n",
    "'''\n",
    "def plot_stype(df):\n",
    "    sit_down = df[df['seating_type'] == 'Sit Down']\n",
    "    sit_down = sit_down['seating_type']\n",
    "    stypes = df.groupby('seating_type').size().reset_index()\n",
    "    stypes.rename(columns={0:'Count'},inplace=True)\n",
    "    print(stypes.Count)\n",
    "    \n",
    "    susp = df[df['seating_type'] == 'Suspended']\n",
    "    susp = susp['seating_type']\n",
    "    counts = [len(susp), len(sit_down)]\n",
    "    labels = ['suspended', 'sit_down']\n",
    "    plt.figure(figsize=(9,9))\n",
    "    plt.pie(stypes.Count,autopct='%0.1f%%',labels=stypes.seating_type,rotatelabels=True,pctdistance=.95)\n",
    "    plt.axis('equal')\n",
    "    #plt.pie(counts,autopct='%0.1f%%', labels=labels)\n",
    "    plt.title(\"Most popular coaster seating types\")\n",
    "    #plt.axis('equal')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "def PlotSeatingData(data,col_name):\n",
    "    seating_data = data[(data.seating_type == 'Sit Down') & (data[col_name] != 0 )].dropna().reset_index()\n",
    "    inverted_data = data[(data.seating_type == 'Inverted') & (data[col_name] != 0 )].dropna().reset_index()\n",
    "    spinning_data = data[(data.seating_type == 'Spinning') & (data[col_name] != 0 )].dropna().reset_index()\n",
    "    ax = plt.subplot()\n",
    "    plt.bar(range(3), [seating_data[col_name].mean(), inverted_data[col_name].mean(),spinning_data[col_name].mean()] )\n",
    "    plt.title('roller coaster seating types')\n",
    "    plt.ylabel('average ' + col_name )\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_xticklabels(['Sit down','Inverted', 'Spinning'])\n",
    "    plt.show()\n",
    "    #print(sit_down)\n",
    "#plot_bar(df2,'Parc Asterix')\n",
    "#plot_hist(df2,\"speed\")\n",
    "#plot_pie(df2)\n",
    "#plot_scatter(df2,'speed','height')\n",
    "#plot_stype(df2)\n",
    "PlotSeatingData(df2,'length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame([['jan', 300],\n",
    "                  ['feb', 290],\n",
    "                  ['mar', 350]])\n",
    "df.columns = ['Month', 'revenue']\n",
    "\n",
    "df2 = pd.DataFrame([['jan', 350],\n",
    "                  ['feb', 280],\n",
    "                  ['mar', 320]])\n",
    "df2.columns = ['Month', 'target']\n",
    "\n",
    "\n",
    "df3 = pd.merge(df,df2)\n",
    "print(df3)\n",
    "\n",
    "crushing_it = df3[df3['revenue'] > df3['target']]\n",
    "print(crushing_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "a = np.random.normal(loc=64, scale=2, size=10000)\n",
    "b = np.random.normal(loc=70, scale=2, size=100000)\n",
    "\n",
    "plt.hist(a, range=(55, 75), bins=20, alpha=0.5)\n",
    "plt.hist(b, range=(55, 75), bins=20, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame([[10310,'Lauren Durham',19,43],\n",
    "                  [18656,'Grace Sellers',10,40],\n",
    "                  [61254,'Shirley Rasmussen',16,30]], columns=['id','name','hourly_wage','hours_worked'])\n",
    "\n",
    "print(df['name'])\n",
    "get_last_name = df.name.apply(lambda lname: lname.split(' ')[-1])\n",
    "\n",
    "df['last_name'] =  df.name.apply(lambda lname: lname.split(' ')[-1])\n",
    "\n",
    "df['total_earned'] = df.apply(lambda row: row['hours_worked'] * row['hourly_wage']   if row['hours_worked']  <= 40 else (row['hourly_wage'] * 40 ) \\\n",
    "                        + (row['hours_worked'] - 40 ) * (row['hourly_wage'] * 1.5 ) , axis=1 )\n",
    "\n",
    "\n",
    "#total_earned = df.apply(lambda row: row['id'],axis=1)\n",
    "\n",
    "df = pd.DataFrame({'name': ['luis','pablo','pedro'],\n",
    "                   'age': ['21','22','23']})\n",
    "\n",
    "df.rename(columns={'name': 'fname',\n",
    "                  'age': 'edad'},inplace=True)\n",
    "\n",
    "print(get_last_name)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "#mylambda = lambda x: x**2 _ 3*x if x > 7 else \n",
    "#print(mylambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "grades = [88, 82, 85, 84, 90]\n",
    "mean = np.mean(grades)\n",
    "\n",
    "difference_one = 88 - mean\n",
    "difference_two = 82 - mean\n",
    "difference_three = 85 - mean\n",
    "difference_four = 84 - mean\n",
    "difference_five = 90 - mean\n",
    "\n",
    "print(difference_five)\n",
    "#Part 1: Sum the differences\n",
    "difference_sum = difference_one + difference_two + difference_three + difference_four + difference_five\n",
    "#print(difference_sum)\n",
    "#Part 2: Average the differences\n",
    "average_difference = float(difference_sum / len(grades))\n",
    "\n",
    "#IGNORE CODE BELOW HERE\n",
    "print(\"The sum of the differences is \" + str(format(difference_sum, \"f\")))\n",
    "print(\"The average difference is \" + str(format(average_difference, \"f\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('threat_feed__all.csv')\n",
    "\n",
    "\n",
    "#print(data.head())\n",
    "data['file'] = data.file.apply(lambda x: \"test\" if not x)\n",
    "print(data.head())\n",
    "print(data[data['tlp'] == 'Green'].iloc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacenters = ['ssw123.chi1', 'fww123.chi1', 'ssw123.phx2', 'fww123.phx2']\n",
    "\n",
    "results_dict = {} \n",
    "for d in datacenters:\n",
    "    datacenter = d.split('.')[1]\n",
    "    device_type = d[-1]\n",
    "    results_dict[datacenter] = {} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('intelbot_output.csv')\n",
    "df = data[data['Abuse_db_confidence_score'] > 0]\n",
    "print(' OR '.join(df['_observable'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pprint\n",
    "\n",
    "with open(\"data.json\",'r') as fd:\n",
    "        \n",
    "    data = json.load(fd)\n",
    "    pprint.pprint(len(data['rl']['malware_uri_feed']['entries']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROR(x, n, bits = 32):\n",
    "    mask = (2**n) - 1\n",
    "    mask_bits = x & mask\n",
    "    return (x >> n) | (mask_bits << (bits - n))\n",
    "def ROL(x, n, bits = 32):\n",
    "    return ROR(x, bits - n, bits)\n",
    "\n",
    "rol = lambda val, r_bits, max_bits=8: \\\n",
    "    (val << r_bits%max_bits) & (2**max_bits-1) | \\\n",
    "    ((val & (2**max_bits-1)) >> (max_bits-(r_bits%max_bits)))\n",
    "# Rotate right. Set max_bits to 8.\n",
    "ror = lambda val, r_bits, max_bits=8: \\\n",
    "    ((val & (2**max_bits-1)) >> r_bits%max_bits) | \\\n",
    "    (val << (max_bits-(r_bits%max_bits)) & (2**max_bits-1))\n",
    "\n",
    "for i in range(1,256):\n",
    "    key = int(0xc7)\n",
    "    \n",
    "    xor = i ^ key\n",
    "    rl = rol(xor,key)\n",
    "    sub =  key- rl\n",
    "    #print(sub)\n",
    "    if sub == 80:\n",
    "        print(\"found\")\n",
    "        print(i)\n",
    "    #rol =  ROL(xor,key)\n",
    "    \n",
    "    \n",
    "# Rotate left. Set max_bits to 8.\n",
    "\n",
    "\n",
    "#print(chr(rol(,190)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
